{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Indexing the Tiff files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. choose the folder where the reef DEMs are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## enter the folder path to the tif files you are analyzing\n",
    "src_path = r\"C:\\Users\\ageglio\\Documents\\NLM_DataRelease\\NLM_DataRelease\"\n",
    "# src_path = r\"C:\\Users\\ageglio\\OneDrive - DOI\\Documents - Reef Mapping\\Data Release\\NLM_DataRelease\\Updated_DataRelease_Surfaces\"\n",
    "\n",
    "## Pattern to filter the bathy tiffs based on their filenames\n",
    "pattern1 = '.*_BY_0.5m'\n",
    "pattern2 = None\n",
    "# pattern2 = '.*0.5m_BY'\n",
    "\n",
    "# read in the directories bathy files and filter\n",
    "NLM_files = Utils.list_files(src_path, '.tif')\n",
    "\n",
    "# Filter based on the provided patterns\n",
    "if pattern1 and pattern2:\n",
    "    NLM_files = [i for i in NLM_files if re.search(pattern1, i) or re.search(pattern2, i)]\n",
    "elif pattern1:\n",
    "    NLM_files = [i for i in NLM_files if re.search(pattern1, i)]\n",
    "elif pattern2:\n",
    "    NLM_files = [i for i in NLM_files if re.search(pattern2, i)]\n",
    "print(\"total number of tiffs indexed from filepath\", len(NLM_files)) # 18\n",
    "\n",
    "# custom function to calculate the extents of the tif files\n",
    "df_tiffs = GetCoordinates().return_min_max_tif_df(NLM_files)\n",
    "df_tiffs.to_csv(\"reef_coordinates.csv\")\n",
    "print(\"filtered tiffs to one per reef totaling: \", len(df_tiffs)) # 18\n",
    "df_tiffs.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Choose a TIF file contained in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manual input of the DEM to be used\n",
    "# input_dem = \"dem\\dem.tif\" #<------------- Large \n",
    "\n",
    "## TO INDEX THE the df_tiffs dataframe to the reef you are interested in, change the index id below\n",
    "input_dem = df_tiffs.filepath[4] #<------------- change index id\n",
    "\n",
    "# Define the products to be created\n",
    "products = [\"dem\"] # just to chunk and merge the dem for formatting reasons\n",
    "products = ['lbp-3-1', 'lbp-15-2', 'lbp-21-3']\n",
    "products = [\"tpi\"]\n",
    "products = [\"slope\", \"aspect\", \"roughness\", \"tpi\", \"tri\", \"hillshade\"]\n",
    "products = [\"shannon_index\"]\n",
    "# products = [\"dem\"]\n",
    "print(input_dem)\n",
    "print(\"products to be created\", products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Derived DEM products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Export habitat derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder path for habitat derivates\n",
    "out_folder = \"habitat_derivatives\"\n",
    "if not os.path.exists(out_folder): os.makedirs(out_folder)\n",
    "\n",
    "# Get input DEM file and name\n",
    "dem_name = Utils().sanitize_path_to_name(input_dem)\n",
    "print(\"creating habitat derivatives for \", dem_name)\n",
    "out_dem_folder = os.path.join(out_folder, dem_name)\n",
    "if not os.path.exists(out_dem_folder): os.mkdir(out_dem_folder)\n",
    "\n",
    "# Generate output file paths\n",
    "output_files = {key: os.path.join(out_dem_folder, dem_name +\"_\"+ key + \".tif\") for key in products}\n",
    "\n",
    "# Process DEM\n",
    "# Dynamically pass output file paths based on the keys in the output_files dictionary\n",
    "HabitatDerivatives(chunk_size=None, use_gdal=True).process_dem(\n",
    "    input_dem,\n",
    "    shannon_window=21,\n",
    "    output_slope=output_files.get(\"slope\"),\n",
    "    output_aspect=output_files.get(\"aspect\"),\n",
    "    output_roughness=output_files.get(\"roughness\"),\n",
    "    output_tpi=output_files.get(\"tpi\"),\n",
    "    output_tri=output_files.get(\"tri\"),\n",
    "    output_hillshade=output_files.get(\"hillshade\"),\n",
    "    output_shannon_index=output_files.get(\"shannon_index\"),\n",
    "    output_lbp_3_1=output_files.get(\"lbp-3-1\"),\n",
    "    output_lbp_15_2=output_files.get(\"lbp-15-2\"),\n",
    "    output_lbp_21_3=output_files.get(\"lbp-21-3\"),\n",
    "    output_dem=output_files.get(\"dem\"),\n",
    ")\n",
    "\n",
    "# Perform Infilling\n",
    "inpainter_ = inpainter(input_dem)\n",
    "binary_mask = inpainter_.get_filled_data_boundary(\n",
    "        shrink_pixels=3, min_area=1000, max_iterations=2, smoothing_radius=3)\n",
    "\n",
    "# Trim Shannon Index\n",
    "shannon_dem = os.path.join(out_dem_folder, dem_name + \"_shannon_index_gdal.tif\")\n",
    "inpainter_.trim_raster(shannon_dem, binary_mask, remove_original=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Create geomorphons landforms using arcpy 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create original landforms from ArcGIS Pro\n",
    "## Note that once is this the procuct is generated in the local workspace, Arc GIS Pro will not allow you to overwrite the file.\n",
    "## This is a limitation of the ArcGIS Pro tool, so you will need to delete the file in the local workspace before running this code again.\n",
    "## Alternatively, you can comment out the line below and just generate the landforms in the next step.\n",
    "landforms(input_dem).calculate_geomorphon_landforms()\n",
    "\n",
    "# calculate the 10class solution\n",
    "output_file10c = landforms(input_dem).classify_bathymorphons(classes=\"10c\")\n",
    "print(f\"Modified raster data saved to {output_file10c}\")\n",
    "\n",
    "# calculate the 6class solution\n",
    "output_file6c = landforms(input_dem).classify_bathymorphons(classes=\"6c\")\n",
    "print(f\"Modified raster data saved to {output_file6c}\")\n",
    "\n",
    "# calculate the 5class solution\n",
    "output_file5c = landforms(input_dem).classify_bathymorphons(classes=\"5c\")\n",
    "print(f\"Modified raster data saved to {output_file5c}\")\n",
    "\n",
    "# calculate the 4class solution\n",
    "output_file4c = landforms(input_dem).classify_bathymorphons(classes=\"4c\")\n",
    "print(f\"Modified raster data saved to {output_file4c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to save the landforms classification histogram and metadata\n",
    "landforms.analyze_raster_data(output_file6c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tracklines to shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. choose a tracklines folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString, Point, Polygon\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input path to tracklines folders from Qimera\n",
    "tracklines_folders_paths = r\"C:\\Users\\ageglio\\OneDrive - DOI\\Documents - Reef Mapping\\Tracks_Data_Release_NLMI_Reefs\\Tracklines\\*\\*\"\n",
    "tracklines_folders = glob.glob(tracklines_folders_paths)\n",
    "\n",
    "# Define the output folder path for shapefiles\n",
    "out_folder = \"shapefiles\"\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "trackline_folder = tracklines_folders[4] #<------------------------ CHANGE INDEX ID\n",
    "print(\"folder path chosen: \", trackline_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. combine and convert tracklines to a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tracklines(trackline_folder):\n",
    "    tracklines_files = glob.glob(os.path.join(trackline_folder, \"*.txt\"))\n",
    "    reef_name = os.path.basename(trackline_folder)\n",
    "    print(\"creating tracklines shapefile for: \", reef_name)\n",
    "    output_shapefile_folder = os.path.join(out_folder, reef_name)\n",
    "    os.makedirs(output_shapefile_folder, exist_ok=True)\n",
    "    output_shapefile = os.path.join(output_shapefile_folder, f\"{reef_name}.shp\")\n",
    "\n",
    "    # Concatenate all dataframes in the list into a single dataframe\n",
    "    tracklines = pd.concat([pd.read_csv(file, delimiter=',', header=None) for file in tracklines_files], ignore_index=True)\n",
    "    tracklines.columns = [\"UTC\", \"X\", \"Y\", \"Delta\"]\n",
    "\n",
    "    # Get the lat lon coordinates of the raw combined tracklines file\n",
    "    xy = GetCoordinates.convert_tracklines_to_lat_lon(tracklines, from_wkt=\"wgs84_16N_OGC_WKT.txt\", wgs84_wkt=\"wgs84_OGC_WKT.txt\")\n",
    "\n",
    "    # Create a GeoDataFrame and save to a shapefile\n",
    "    gpd.GeoDataFrame(geometry=[LineString(xy)], crs=\"EPSG:4326\").to_file(output_shapefile)\n",
    "\n",
    "create_tracklines(trackline_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcpy3.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
